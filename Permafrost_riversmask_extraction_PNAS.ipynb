{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "upset-diagnosis",
   "metadata": {},
   "source": [
    "# River masks Semi-authomatic Multi-Temporal extraction from Landsat SR images usign Google Earth Engine\n",
    "### Variant of the original code that uses a service account"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "russian-pledge",
   "metadata": {},
   "source": [
    "### Niccolò Ragno, Riccardo Bonanomi, Marta Crivellaro, Alfonso Vitti, Guido Zolezzi and Marco Tubino\n",
    "* Publication corresponding Author: Niccolò Ragno,  niccolo.ragno@unitn.it\n",
    "* GEE code corresponding Author: Marta Crivellaro,  marta.crivellaro@unitn.it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e238c30-9946-4565-a138-23b468ca74a5",
   "metadata": {},
   "source": [
    "GEE Python installation: https://developers.google.com/earth-engine/guides/python_install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complimentary-pasta",
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries import\n",
    "import os, sys, glob, math,subprocess,tarfile,shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import linalg as la\n",
    "from functions import ndvi,mndwi,addindex,areaImg,maxValue,ndviMap,mndwiMap\n",
    "import ee\n",
    "import geemap\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc1ecf5-41e6-43b5-875b-5919968b0438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Earth Engine authentication - see readme for details\n",
    "from credentials_script import get_credentials\n",
    "service_account, credentials_folder, credentials_file = get_credentials()\n",
    "credentials = ee.ServiceAccountCredentials(service_account, credentials_folder + credentials_file)\n",
    "ee.Initialize(credentials)\n",
    "\n",
    "# Google drive autentiucation - see readme for details\n",
    "gauth = GoogleAuth()\n",
    "scopes = ['https://www.googleapis.com/auth/drive']\n",
    "gauth.credentials = ServiceAccountCredentials.from_json_keyfile_name(credentials_folder+credentials_file, scopes=scopes)\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the input data\n",
    "river_name = 'Kuk'\n",
    "\n",
    "#1 - Define region(S) of interest (roi) as rectangular extents\n",
    "#EPSG: 32604 Kuk Alaska\n",
    "river_geom = ee.Geometry.Polygon([\n",
    "    [[-159.2000, 70.0365],\n",
    "     [-159.0765, 70.2179],\n",
    "     [-159.6402, 70.1556],\n",
    "     [-159.7777, 70.0919],\n",
    "     [-159.2000, 70.0365]]])\n",
    "EPSG_code = 'EPSG:32604'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926fb3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output data folder\n",
    "# local\n",
    "output_folder = 'D:/Documents/PhD/Codici/PyRIS/GEE-masks/output/' + river_name + '/'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Google Drive\n",
    "PM_drive_folder = river_name + '-' + 'Permafrost_rivermask_PM'\n",
    "regular_drive_folder = river_name + '-' + 'Permafrost_rivermask'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-thanksgiving",
   "metadata": {},
   "source": [
    "### Create an interactive map\n",
    "The default basemap is _Google Maps_. Additional basemaps can be added using the ``Map.add_basemap()`` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pending-beast",
   "metadata": {},
   "outputs": [],
   "source": [
    "Map = geemap.Map(center=[19.76,40.41], zoom=22)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d64790-4b5c-481c-9067-fa3027331103",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcGeom = river_geom\n",
    "roi = fcGeom \n",
    "Map.centerObject(roi)\n",
    "Map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-illustration",
   "metadata": {},
   "source": [
    "Standardise band names, merge Landsat data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-slope",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardise band names, merge Landsat data:\n",
    "bn8 = ['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B6', 'SR_QA_AEROSOL', 'SR_B5', 'SR_B7', 'QA_PIXEL']\n",
    "bn7 = ['SR_B1', 'SR_B1', 'SR_B2', 'SR_B3', 'SR_B5', 'SR_CLOUD_QA', 'SR_B4', 'SR_B7','QA_PIXEL']\n",
    "bn5 = ['SR_B1', 'SR_B1', 'SR_B2', 'SR_B3', 'SR_B5', 'SR_CLOUD_QA', 'SR_B4', 'SR_B7','QA_PIXEL']\n",
    "#standard bans:\n",
    "bnL = ['uBlue', 'Blue', 'Green', 'Red', 'Swir1', 'BQA', 'Nir', 'Swir2','qa_pixel']\n",
    "\n",
    "## defining cloudmask function for landsat 7 and 8 only\n",
    "# This function masks the input with a threshold on the simple cloud score.\n",
    "# Observe that the input to simpleCloudScore() is a single Landsat TOA scene. \n",
    "# Also note that simpleCloudScore() adds a band called ‘cloud’ to the input image. \n",
    "# The cloud band contains the cloud score from 0 (not cloudy) to 100 (most cloudy).\n",
    "def cloudMask(img):\n",
    "    cloudscore = ee.Algorithms.Landsat.simpleCloudScore(img).select('cloud')\n",
    "    return img.updateMask(cloudscore.lt(10))\n",
    "\n",
    "def maskClouds(image):\n",
    "    \n",
    "    cloudShadowBitMask = (1 << 3)\n",
    "    cloudsBitMask = (1 << 5)\n",
    "    \n",
    "    qa = image.select('qa_pixel')\n",
    "    mask = (qa.bitwiseAnd(cloudShadowBitMask).eq(0).And(qa.bitwiseAnd(cloudsBitMask).eq(0)))\n",
    "    \n",
    "    return image.updateMask(mask)\n",
    "\n",
    "#calling LS Surface Reflectance image collections \n",
    "ls5 = ee.ImageCollection(\"LANDSAT/LT05/C02/T1_L2\").filter(ee.Filter.lt('CLOUD_COVER',15)).select(bn5, bnL)\n",
    "ls7 = (ee.ImageCollection(\"LANDSAT/LE07/C02/T1_L2\")  \\\n",
    "  .map(cloudMask)  \\\n",
    "  .filterDate('1999-04-15', '2003-05-30')  \\\n",
    "  .select(bn7, bnL))\n",
    "ls8 = ee.ImageCollection(\"LANDSAT/LC08/C02/T1_L2\").filter(ee.Filter.lt('CLOUD_COVER', 15)).select(bn8, bnL)\n",
    "#merging LS 5 and 8 dataset\n",
    "ls = ls5.merge(ls8).sort('system:start', True).filter(ee.Filter.calendarRange(5,9,'month'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c92f4c-2058-4cec-9589-5d5dc8326428",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions definition\n",
    "def ErosDil(image):\n",
    "    kernel = ee.Kernel.square(2)\n",
    "    opened = image.focalMin(kernel=kernel, iterations=2).focalMax(kernel=kernel, iterations=2)\n",
    "    return opened\n",
    "\n",
    "def ClassifyWater(imgIn, method = 'Jones2019'):\n",
    "    if method == 'Jones2019':\n",
    "        from functions_waterClassification_Jones2019median import ClassifyWaterJones2019\n",
    "        return(ClassifyWaterJones2019(imgIn))\n",
    "    elif method == 'Zou2018':\n",
    "        from functions_waterClassification_Zou2018median import ClassifyWaterZou2018\n",
    "        return(ClassifyWaterZou2018(imgIn))\n",
    "    \n",
    "def RGBtoHSV (Image):\n",
    "    sat = Image.select(['Red','Green','Blue']).divide(65455).rgbToHsv().select(['saturation'])\n",
    "    return Image.addBands(sat)\n",
    "\n",
    "def histogram(image):\n",
    "    # Compute the histogram of the NIR band.  The mean and variance are only FYI.\n",
    "    polygon = ee.Geometry(image.geometry())\n",
    "    histogram = image.reduceRegion(\n",
    "        **{\n",
    "            'reducer': ee.Reducer.histogram(255, 2),\n",
    "            'geometry': polygon,\n",
    "            'scale': 15,\n",
    "            'bestEffort': True,\n",
    "        }\n",
    "    )\n",
    "    return histogram\n",
    "\n",
    "# Return the DN that maximizes interclass variance in B5 (in the region).\n",
    "def otsu(histogram):\n",
    "    counts = ee.Array(ee.Dictionary(histogram).get('histogram'))\n",
    "    means = ee.Array(ee.Dictionary(histogram).get('bucketMeans'))\n",
    "    size = means.length().get([0])\n",
    "    total = counts.reduce(ee.Reducer.sum(), [0]).get([0])\n",
    "    sum = means.multiply(counts).reduce(ee.Reducer.sum(), [0]).get([0])\n",
    "    mean = sum.divide(total)\n",
    "\n",
    "    indices = ee.List.sequence(1, size)\n",
    "\n",
    "    # Compute between sum of squares, where each mean partitions the data.\n",
    "\n",
    "    def func_xxx(i):\n",
    "        aCounts = counts.slice(0, 0, i)\n",
    "        aCount = aCounts.reduce(ee.Reducer.sum(), [0]).get([0])\n",
    "        aMeans = means.slice(0, 0, i)\n",
    "        aMean = (\n",
    "            aMeans.multiply(aCounts)\n",
    "            .reduce(ee.Reducer.sum(), [0])\n",
    "            .get([0])\n",
    "            .divide(aCount)\n",
    "        )\n",
    "        bCount = total.subtract(aCount)\n",
    "        bMean = sum.subtract(aCount.multiply(aMean)).divide(bCount)\n",
    "        return aCount.multiply(aMean.subtract(mean).pow(2)).add(\n",
    "            bCount.multiply(bMean.subtract(mean).pow(2))\n",
    "        )\n",
    "\n",
    "    bss = indices.map(func_xxx)\n",
    "\n",
    "    # Return the mean value corresponding to the maximum BSS.\n",
    "    return means.sort(bss).get([-1])\n",
    "\n",
    "def extract_ac(imagesat,imagemndwi, t_sat, t_mndwi):\n",
    "    ac = imagesat.select('saturation_median').gte(t_sat).Or(imagemndwi.select('mndwi_median').gte(t_mndwi)).selfMask()\n",
    "    return ee.Image(ac)\n",
    "\n",
    "def stdLocal (image, roi): \n",
    "    geom = roi.geometry()\n",
    "    std_value = image.clip(geom).reduceRegion(**{\n",
    "        'reducer': ee.Reducer.stdDev(),\n",
    "        'geometry': geom,\n",
    "        'scale': 30,\n",
    "        'maxPixels': 1e12,\n",
    "        'tileScale': 16\n",
    "    })\n",
    "    return std_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003ef720-11a5-4382-9087-325248283d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PERONA MALIK FILTER\n",
    "# Perona malik filter\n",
    "# I(n+1, i, j) = I(n, i, j) + Lambda * (cN * dN(I) + cS * dS(I) + cE * dE(I), cW * dW(I))\n",
    "#**\n",
    "#Perona-Malik (anisotropic diffusion) convolution\n",
    "#by Gennadii Donchyts see https://groups.google.com/forum/#!topic/google-earth-engine-developers/a9W0Nlrhoq0\n",
    "#I(n+1, i, j) = I(n, i, j) + lambda * (cN * dN(I) + cS * dS(I) + cE * dE(I), cW * dW(I))\n",
    "#iter: Number of interations to apply filter\n",
    "#K: kernal size\n",
    "#method: choose method 1 (default) or 2\n",
    "# Returns: image \n",
    "#\n",
    "def peronaMalikFilter(I, iter, K, method, l):\n",
    "    dxW = ee.Kernel.fixed(3, 3, [[ 0,  0,  0], [ 1, -1,  0], [ 0,  0,  0]])\n",
    "    dxE = ee.Kernel.fixed(3, 3, [[ 0,  0,  0], [ 0, -1,  1], [ 0,  0,  0]])\n",
    "    dyN = ee.Kernel.fixed(3, 3, [[ 0,  1,  0], [ 0, -1,  0], [ 0,  0,  0]])\n",
    "    dyS = ee.Kernel.fixed(3, 3, [[ 0,  0,  0], [ 0, -1,  0], [ 0,  1,  0]])\n",
    "    \n",
    "    Lambda = l \n",
    "    \n",
    "    k1 = ee.Image(-1.0/K)\n",
    "    k2 = ee.Image(K).multiply(ee.Image(K))\n",
    "    \n",
    "    for i in range(0, iter):\n",
    "        dI_W = I.convolve(dxW)\n",
    "        dI_E = I.convolve(dxE)\n",
    "        dI_N = I.convolve(dyN)\n",
    "        dI_S = I.convolve(dyS)\n",
    "        \n",
    "        if method == 1:\n",
    "            cW = dI_W.multiply(dI_W).multiply(k1).exp()\n",
    "            cE = dI_E.multiply(dI_E).multiply(k1).exp()\n",
    "            cN = dI_N.multiply(dI_N).multiply(k1).exp()\n",
    "            cS = dI_S.multiply(dI_S).multiply(k1).exp()\n",
    "        elif method == 2:\n",
    "            cW = ee.Image(1.0).divide(ee.Image(1.0).add(dI_W.multiply(dI_W).divide(k2)))\n",
    "            cE = ee.Image(1.0).divide(ee.Image(1.0).add(dI_E.multiply(dI_E).divide(k2)))\n",
    "            cN = ee.Image(1.0).divide(ee.Image(1.0).add(dI_N.multiply(dI_N).divide(k2)))\n",
    "            cS = ee.Image(1.0).divide(ee.Image(1.0).add(dI_S.multiply(dI_S).divide(k2)))\n",
    "        I = I.add(ee.Image(Lambda).multiply(cN.multiply(dI_N).add(cS.multiply(dI_S)).add(cE.multiply(dI_E)).add(cW.multiply(dI_W))))\n",
    "    \n",
    "    return I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882a04f9-56d1-454b-8590-3add989d15cc",
   "metadata": {},
   "source": [
    "### Cycle on years to export annual domain mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e512ca3d-3c0c-42f8-bea1-2a8081476b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = [1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993,\n",
    "         1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003,\n",
    "         2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013,\n",
    "         2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023]\n",
    "dates_out = dates[:]\n",
    "ACy = {}\n",
    "data= []\n",
    "\n",
    "roi = fcGeom\n",
    "for year in dates:\n",
    "    sDate_T1 = str(year)+\"-05-01\"; \n",
    "    eDate_T1 = str(year)+\"-09-30\";\n",
    "    #Sort by:  roi, date:\n",
    "    collection= ls \\\n",
    "        .filterBounds(roi) \\\n",
    "        .sort('system:start', True) \\\n",
    "        .filterDate(sDate_T1,eDate_T1)\n",
    "\n",
    "    # Create a list of image objects.\n",
    "    imageList = collection.toList(100);\n",
    "    Collection = collection.map(ndviMap).map(mndwiMap).map(RGBtoHSV)\n",
    "    median = Collection.reduce(ee.Reducer.median())#.reproject(ee.Projection(EPSG_code)) #qui è bene cambiare con la proiezione WGS 84 UTM LOCALE\n",
    "    maxi = Collection.reduce(ee.Reducer.percentile([90]))#.reproject(ee.Projection(EPSG_code))\n",
    "    mini = Collection.reduce(ee.Reducer.min())#.reproject(ee.Projection(EPSG_code))\n",
    "\n",
    "    ndvimap_median=median.select('ndvi_median').clip(roi)\n",
    "    ndvimap_90p=maxi.select('ndvi_p90').clip(roi)\n",
    "    mndwimap_median=median.select('mndwi_median').clip(roi)\n",
    "    \n",
    "    satmap_90p=maxi.select('saturation_p90').clip(roi)\n",
    "   \n",
    "    ndwimap_90p=maxi.select('mndwi_p90').clip(roi)\n",
    "\n",
    "\n",
    "    pm_mndwi_0_3 = peronaMalikFilter(mndwimap_median, 5, 2, 1, 0.3)\n",
    "  \n",
    "    otsu_mndwi = otsu(histogram(mndwimap_median).get('mndwi_median'))\n",
    "    otsu_sat = otsu(histogram(satmap_90p).get('saturation_p90'))\n",
    "    otsu_mndwiPM = otsu(histogram(pm_mndwi_0_3).get('mndwi_median'))\n",
    "    \n",
    "    veg1 = ndvimap_90p.select('ndvi_p90').gte(0.15)\n",
    "    water3 =  satmap_90p.gt(otsu_sat).Or(ee.Image(mndwimap_median.select('mndwi_median')\n",
    "                                                  .gte(otsu_mndwi)).And(veg1.lt(1)))\n",
    "\n",
    "    waterPM3 = satmap_90p.gt(otsu_sat).Or(pm_mndwi_0_3.select('mndwi_median')\n",
    "                                          .gte(otsu_mndwiPM)).And(veg1.lt(1))\n",
    "\n",
    "    area_raw_N = areaImg(water3.remap(ee.List([0]),ee.List([1])))\n",
    "    area_pm_N = areaImg(waterPM3.remap(ee.List([0]),ee.List([1])))\n",
    "    \n",
    "    try:\n",
    "        # extract the value as a number\n",
    "        area_raw_number = area_raw_N.getNumber('remapped').getInfo()\n",
    "        area_pm_number = area_pm_N.getNumber('remapped').getInfo()\n",
    "        #dataframe with extracted areas and thresholds info creation and compiling\n",
    "        data.append(dict(zip(('year','area_raw_number','area_pm_number','%d','t_nmndwi','t_PMmndwi'),\n",
    "                            (str(year),area_raw_number,area_pm_number,(100*((area_raw_number-area_pm_number)/area_raw_number)),otsu_mndwi.getInfo(),otsu_mndwiPM.getInfo(),))))\n",
    "        \n",
    "        #Export the images, specifying scale and region.\n",
    "        task = ee.batch.Export.image.toDrive(**{\n",
    "                'image': waterPM3.clip(fcGeom),\n",
    "                'description': river_name + str(year),\n",
    "                'folder': PM_drive_folder,\n",
    "                'scale': 30,\n",
    "                'crs': EPSG_code,\n",
    "                'region': fcGeom\n",
    "\n",
    "            })\n",
    "        task.start()\n",
    "\n",
    "        task = ee.batch.Export.image.toDrive(**{\n",
    "                'image': water3.clip(fcGeom),\n",
    "                'description': river_name + str(year),\n",
    "                'folder': regular_drive_folder,\n",
    "                'scale': 30,\n",
    "                'crs': EPSG_code,\n",
    "                'region': fcGeom\n",
    "\n",
    "            })\n",
    "        task.start()\n",
    "                    \n",
    "        print(str(year) + ' Done!')\n",
    "    except:\n",
    "        print(str(year) + ' Error!')\n",
    "        dates_out.remove(year)\n",
    "\n",
    "print('The year where there are data are:')\n",
    "print(dates_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969ca477-3616-444c-9e88-843f677881c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the dataframe as csv file\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(output_folder + river_name + '_stats.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if exporting task are yet to start or are still running\n",
    "export_task  = [dict for dict in ee.data.listOperations() if dict['metadata']['type'] == 'EXPORT_IMAGE']\n",
    "running_task = [dict for dict in export_task if dict['metadata']['state'] == 'RUNNING']\n",
    "ready_task   = [dict for dict in export_task if dict['metadata']['state'] == 'READY']\n",
    "\n",
    "while not (not running_task and not ready_task):\n",
    "    print('There are still %d running tasks and %d task to start. Cheching again in a minute.', (len(running_task), len(ready_task)))    \n",
    "    time.sleep(60)\n",
    "\n",
    "# download the files from Google Drive and delete them afterwards\n",
    "# list all folders in the root Drive folder\n",
    "folder_list = drive.ListFile({'q': \"'root' in parents and trashed=false\"}).GetList()\n",
    "\n",
    "if not folder_list:\n",
    "    print('No folders found in the root folder')\n",
    "else:\n",
    "    print('Downloading files from Google Drive')\n",
    "\n",
    "    # iterate over all folders\n",
    "    for folder in folder_list:\n",
    "        if folder['title'] == regular_drive_folder:\n",
    "            saving_folder = output_folder + 'rivermask/'\n",
    "        elif folder['title'] == PM_drive_folder:\n",
    "            saving_folder = output_folder + 'rivermask_PM/'\n",
    "        else:\n",
    "            print('Deleting folder: %s, id: %s' % (folder['title'], folder['id']))\n",
    "            folder.Delete()\n",
    "            continue\n",
    "        \n",
    "        os.makedirs(saving_folder, exist_ok=True)\n",
    "        print('\\ntitle: %s, id: %s' % (folder['title'], folder['id']))\n",
    "\n",
    "        # list and download all files in the folder\n",
    "        file_list = drive.ListFile({'q': \"'%s' in parents and trashed=false\" % folder['id']}).GetList()\n",
    "\n",
    "        for file in file_list:\n",
    "            filename = file['title']\n",
    "            print('title: %s, id: %s' % (file['title'], file['id']))\n",
    "            \n",
    "            # download file into working directory (in this case a tiff-file)\n",
    "            file.GetContentFile(saving_folder + filename, mimetype=\"image/tiff\")\n",
    "\n",
    "            # delete file afterwards to keep the Drive empty\n",
    "            file.Delete()\n",
    "            \n",
    "        folder.Delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
